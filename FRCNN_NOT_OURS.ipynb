{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster RCNN RPN for GTSDB "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is our implementation of a FRCNN RPN for traffic sign detection using the GTSDB dataset\n",
    "\n",
    "Ren, Shaoqing, Kaiming He, Ross Girshick, and Jian Sun. “Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks.” *ArXiv:1506.01497 [Cs]*, June 4, 2015. http://arxiv.org/abs/1506.01497."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, Input\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.callbacks import ProgbarLogger, ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "import keras.backend as K\n",
    "\n",
    "import numpy as np\n",
    "from skimage.io import imread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build base layers (a.k.a. shared layers in the frcnn paper) on the given input layer. The vgg16 network pre-trained on ImageNet was one of the base networks used in the frcnn paper. However, most traffic signs in our datasets are 32x32 in size. Vgg16 has 32x downsample, making it unsuitable for use. The `base` network is a simple network used in our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg(input_layer, trainable=False):\n",
    "    vgg = keras.applications.vgg16.VGG16(include_top=False)\n",
    "    vgg.trainable = trainable\n",
    "    return vgg(input_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base(input_layer):\n",
    "    return Sequential([\n",
    "        Conv2D(filters=32, kernel_size=(3,3), padding=\"same\", activation='relu', input_shape=(800,1360,3)),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(filters=64, kernel_size=(5,5), padding=\"same\", activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "    ])(input_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RPN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build an RPN on the given base layers with k (no. of anchors at each position, as defined in the paper). Returns a 2-tuple of the 2 output layers: cls and regr, for object/no-object classification and bounding box regression respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rpn(base, k):\n",
    "    x = Conv2D(256, (3, 3), padding='same', activation='relu',\n",
    "               kernel_initializer=keras.initializers.RandomNormal(0.0, 0.01), name='rpn_conv_1')(base)\n",
    "    return(\n",
    "        Conv2D(k,   (1, 1), activation='sigmoid',\n",
    "               kernel_initializer=keras.initializers.RandomNormal(0.0, 0.01), name='rpn_cls')(x),\n",
    "        Conv2D(k*4, (1, 1), activation='linear',\n",
    "               kernel_initializer=keras.initializers.RandomNormal(0.0, 0.01), name='rpn_regr')(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility functions for working with boxes. These extensively use numpy array manipulation functions to take advantage of the low-level numpy implementation instead of looping in Python-land."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anchors(rows, cols, sizes, stride):\n",
    "    # Generate 1:1 anchor boxes\n",
    "    # anc_num = len(sizes) [= k in the frcnn paper]\n",
    "    # shape = (rows, cols, anc_num, 4)\n",
    "    return np.expand_dims(np.tile(np.indices((rows,cols)).transpose((1,2,0)) * stride + .5, 2), axis=2).repeat(len(sizes), axis=2)\\\n",
    "        + np.repeat(np.expand_dims(np.array(sizes), axis=1), 4, axis=1) * [-.5, -.5, .5, .5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersect(b1, b2):\n",
    "    # b1.shape = (rows, cols, anc_num, 4)\n",
    "    # b2.shape = (4,)\n",
    "    m = np.minimum(b1,b2)\n",
    "    M = np.maximum(b1,b2)\n",
    "    h = np.maximum(m[...,2] - M[...,0], 0)\n",
    "    w = np.maximum(m[...,3] - M[...,1], 0)\n",
    "    return w*h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def union(b1, b2, iarea):\n",
    "    # b1.shape = (rows, cols, anc_num, 4)\n",
    "    # b2.shape = (4,)\n",
    "    a1 = (b1[...,2]-b1[...,0]) * (b1[...,3]-b1[...,1])\n",
    "    a2 = (b2[...,2]-b2[...,0]) * (b2[...,3]-b2[...,1])\n",
    "    return a1 + a2 - iarea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(b1, b2):\n",
    "    # b1.shape = (rows, cols, anc_num, 4)\n",
    "    # b2.shape = (4,)\n",
    "    iarea = intersect(b1, b2)\n",
    "    uarea = union(b1, b2, iarea)\n",
    "    return iarea/uarea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coords2param(ancs, gtbs):\n",
    "    # Convert absolute coords to parametrized params (see frcnn paper)\n",
    "    # ancs.shape = gtbs.shape = (rows, cols, anc_num, 4)\n",
    "    # box = [y1,x1,y2,x2]\n",
    "    \n",
    "    wa = ancs[...,3] - ancs[...,1]\n",
    "    ha = ancs[...,2] - ancs[...,0]\n",
    "    tx = (gtbs[...,1] - ancs[...,1]) / wa\n",
    "    ty = (gtbs[...,1] - ancs[...,1]) / ha\n",
    "    tw = np.log((gtbs[...,3] - gtbs[...,1]) / wa)\n",
    "    th = np.log((gtbs[...,2] - gtbs[...,0]) / ha)\n",
    "    \n",
    "    # shape = (row, cols, anc_num, 4)\n",
    "    return np.stack([tx,ty,tw,th], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anchors_vs_gt(ancs, gtbs, lo=.3, hi=.7):\n",
    "    # ancs.shape = (rows, cols, anc_num, 4)\n",
    "    # gtbs.shape = (gtb_num, 4)\n",
    "    \n",
    "    # ious.shape = (rows, cols, anc_num, gtb_num)\n",
    "    ious = np.stack([iou(ancs, gtb) for gtb in gtbs], axis=-1)\n",
    "    # best.shape = (gtb_num,)\n",
    "    best = ious.reshape((-1, gtbs.shape[0])).max(axis=0)\n",
    "    \n",
    "    # box_pos.shape = box_neg.shape = (rows, cols, anc_num)\n",
    "    box_pos = np.logical_or(ious.max(axis=-1) >= hi, np.logical_and(ious == best, best > 0).any(axis=-1))\n",
    "    box_neg = ious.max(axis=-1) <= lo\n",
    "    \n",
    "    # hard_pos = anchor boxes with iou >= hi with any gt box\n",
    "    # soft_pos = anchor boxes with highest iou with a gt box\n",
    "    # hard_neg = anchor boxes with iou <= lo with all gt boxes\n",
    "    ## print(\"\\thard_pos = {:d}\".format(np.sum(ious.max(axis=-1) >= hi)))\n",
    "    ## print(\"\\tsoft_pos = {:d}\".format(np.sum(np.logical_and(ious == best, best > 0).any(axis=-1))))\n",
    "    ## print(\"\\thard_neg = {:d}\".format(np.sum(ious.max(axis=-1) <= lo)))\n",
    "    \n",
    "    # best_gt.shape = (rows, cols, anc_num, 4)\n",
    "    best_gt = np.take(gtbs, ious.argmax(axis=-1), axis=0)\n",
    "    \n",
    "    return box_pos, box_neg, coords2param(ancs, best_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_boxes(pos, neg, num=256):\n",
    "    # Only use num boxes, with pos:neg at most 1:1 unless pos < num/2\n",
    "    p_num = pos[pos].shape[0]\n",
    "    n_num = neg[neg].shape[0]\n",
    "    if p_num > num/2:\n",
    "        pos[np.vsplit(np.vstack(np.where(pos))[:,np.random.choice(p_num, p_num-num//2, replace=False)], pos.ndim)] = False\n",
    "        p_num = num//2\n",
    "    if n_num + p_num > num:\n",
    "        neg[np.vsplit(np.vstack(np.where(neg))[:,np.random.choice(n_num, n_num-num+p_num, replace=False)], neg.ndim)] = False\n",
    "    return pos, neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss functions as defined in the frcnn paper:  \n",
    "`L({pi}, {ti}) = (1/Ncls) Σ Lcls(pi, pi*) + λ(1/Nreg) Σ pi* Lreg(ti, ti*)`\n",
    "\n",
    "In this implementation we ignore the balancing factor λ and the normalization, which the paper say is okay:  \n",
    "`L({pi}, {ti}) = Σ Lcls(pi, pi*) + Σ pi* Lreg(ti, ti*)`  \n",
    "where `Lcls` is binary log loss  \n",
    "and `Lreg` is robust loss\n",
    "\n",
    "Here, `rpn_cls_loss` is `Σ Lcls(pi, pi*)`  \n",
    "and `rpn_regr_loss` is `Σ pi* Lreg(ti, ti*)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rpn_regr_loss(num_ancs):\n",
    "    def loss(ytrue, ypred, ptrue):\n",
    "        # ytrue.shape = (rows, cols, num_ancs * 4)\n",
    "        # ypred.shape = (rows, cols, num_ancs * 4)\n",
    "        # ptrue.shape = (rows, cols, num_ancs * 4)\n",
    "        # ancs.shape  = (rows, cols, num_ancs * 4)\n",
    "        \n",
    "        dy = ytrue - ypred\n",
    "        sw = K.cast(K.less(K.abs(dy), 1), dtype=K.floatx())\n",
    "        r1 = sw*dy*dy*.5\n",
    "        r2 = (1-sw)*(K.abs(dy)-.5)\n",
    "        return K.sum((r1+r2) * ptrue)\n",
    "    return lambda ytrue, ypred: \\\n",
    "        loss(ytrue[...,4*num_ancs:],\n",
    "             ypred,\n",
    "             ytrue[...,:4*num_ancs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rpn_cls_loss(num_ancs):\n",
    "    def loss(postrue, negtrue, ppred):\n",
    "        # Add epsilon = 1e-4 to prevent log(0)\n",
    "        return K.sum(- postrue * K.log(1e-4 + ppred)\n",
    "                     - negtrue * K.log(1e-4 + 1-ppred))\n",
    "    return lambda ptrue, ppred: \\\n",
    "        loss(ptrue[...,:num_ancs], ptrue[...,num_ancs:], ppred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data generator. Reads from the GTSDB training set. Generate a mini-batch (one image) each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def datagen(start, stop, ancs=None, shuffle=True):\n",
    "    \"\"\"Generator for GTSDB dataset\n",
    "        Args:\n",
    "            start, stop = range of images to use\n",
    "            ancs = anchor boxes to use. Use get_anchors() to generate these.\n",
    "                Dimensions should be (imageheight/basenet_stride, imagewidht/basenet_stride, k, basenet_stride)\n",
    "                Defaults to be get_anchors(200, 340, [16,24,32], 4)\n",
    "            shuffle = whether to shuffle the data\n",
    "    \"\"\"\n",
    "    if ancs is None:\n",
    "        ancs = get_anchors(200, 340, [16,24,32], 4)\n",
    "    csv = np.loadtxt('datasets/GTSDB/gt.txt', delimiter=';', converters = {0: lambda x:x[:-4]}, dtype=np.float32)\n",
    "    idx = np.arange(start, stop)\n",
    "    \n",
    "    for i in idx:\n",
    "        temp = csv[csv[:,0] == i]\n",
    "        temp = temp[:,[2,1,4,3]]\n",
    "        # temp.shape = (gtb_num, 4)\n",
    "        \n",
    "        ## print(\"fname = ../dataset/PNG_train/{:05d}.png\".format(i))\n",
    "        pos, neg, gtbs = anchors_vs_gt(ancs, temp)\n",
    "        gtbs = gtbs.reshape((gtbs.shape[0], gtbs.shape[1], -1))\n",
    "        # pos.shape = neg.shape = (rows, cols, anc_num)\n",
    "        # gtbs.shape = (row, cols, anc_num * 4)\n",
    "        pos, neg = filter_boxes(pos, neg)\n",
    "        \n",
    "        # x_img.shape  = (1, imgh, imgw, imgchannels)\n",
    "        # y_cls.shape  = (row, cols, anc_num * 2)\n",
    "        # y_regr.shape = (row, cols, anc_num * 8)\n",
    "        x_img  = np.expand_dims(imread('datasets/GTSDB/images/{:05d}.ppm'.format(i)), 0)\n",
    "        y_cls  = np.expand_dims(np.concatenate((pos, neg), axis=-1).astype(np.float32), 0)\n",
    "        y_regr = np.expand_dims(np.concatenate((pos.repeat(4, axis=-1), gtbs), axis=-1), 0)\n",
    "        \n",
    "        yield (x_img, [y_cls, y_regr])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datagen(0, 5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'generator' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-749a7ec1bb63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'generator' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "data[1][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search for models in the checkpoint directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "temp = [re.compile('gtsdb_rpn-(\\d+)\\.hdf5').match(fn) for fn in os.listdir('models/')]\n",
    "temp = [int(m.group(1)) for m in temp if m is not None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If models are found, load the latest model.  \n",
    "Otherwise build new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(temp) == 0:\n",
    "    inp = Input(shape=(800,1360,3))\n",
    "    # Can use vgg(inp) instead of base(inp)\n",
    "    model = Model(inputs=inp, outputs=rpn(base(inp), 3))\n",
    "else:\n",
    "    model = keras.models.load_model('models/gtsdb_rpn-{:d}.hdf5'.format(max(temp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd', loss={'rpn_cls': rpn_cls_loss(3), 'rpn_regr': rpn_regr_loss(3)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 800, 1360, 3 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_6 (Sequential)       (None, 200, 340, 64) 52160       input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "rpn_conv_1 (Conv2D)             (None, 200, 340, 256 147712      sequential_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "rpn_cls (Conv2D)                (None, 200, 340, 3)  771         rpn_conv_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "rpn_regr (Conv2D)               (None, 200, 340, 12) 3084        rpn_conv_1[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 203,727\n",
      "Trainable params: 203,727\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-call train function. Generates checkpoints and tensorboard logs. Early stopping on validation error plateau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`write_grads` will be ignored in TensorFlow 2.0 for the `TensorBoard` Callback.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-384f8c3db02f>:9: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  neg[np.vsplit(np.vstack(np.where(neg))[:,np.random.choice(n_num, n_num-num+p_num, replace=False)], neg.ndim)] = False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  1/598 [..............................] - ETA: 11:34 - loss: nan - rpn_cls_loss: nan - rpn_regr_loss: nan"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-384f8c3db02f>:6: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  pos[np.vsplit(np.vstack(np.where(pos))[:,np.random.choice(p_num, p_num-num//2, replace=False)], pos.ndim)] = False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/598 [====>.........................] - ETA: 9:26 - loss: nan - rpn_cls_loss: nan - rpn_regr_loss: nan"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " ValueError: need at least one array to stack\nTraceback (most recent call last):\n\n  File \"/user/HS127/at01053/.local/lib/python3.8/site-packages/tensorflow/python/ops/script_ops.py\", line 249, in __call__\n    ret = func(*args)\n\n  File \"/user/HS127/at01053/.local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 620, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/user/HS127/at01053/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 891, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/user/HS127/at01053/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\", line 807, in wrapped_generator\n    for data in generator_fn():\n\n  File \"<ipython-input-70-40878a2fbef8>\", line 21, in datagen\n    pos, neg, gtbs = anchors_vs_gt(ancs, temp)\n\n  File \"<ipython-input-13-33dd25bd47a1>\", line 6, in anchors_vs_gt\n    ious = np.stack([iou(ancs, gtb) for gtb in gtbs], axis=-1)\n\n  File \"<__array_function__ internals>\", line 5, in stack\n\n  File \"/user/HS127/at01053/.local/lib/python3.8/site-packages/numpy/core/shape_base.py\", line 423, in stack\n    raise ValueError('need at least one array to stack')\n\nValueError: need at least one array to stack\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_train_function_2529]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-bdc27e78303f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.fit(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdatagen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m598\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m598\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatagen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m599\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m899\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m:  ValueError: need at least one array to stack\nTraceback (most recent call last):\n\n  File \"/user/HS127/at01053/.local/lib/python3.8/site-packages/tensorflow/python/ops/script_ops.py\", line 249, in __call__\n    ret = func(*args)\n\n  File \"/user/HS127/at01053/.local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 620, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/user/HS127/at01053/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 891, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/user/HS127/at01053/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\", line 807, in wrapped_generator\n    for data in generator_fn():\n\n  File \"<ipython-input-70-40878a2fbef8>\", line 21, in datagen\n    pos, neg, gtbs = anchors_vs_gt(ancs, temp)\n\n  File \"<ipython-input-13-33dd25bd47a1>\", line 6, in anchors_vs_gt\n    ious = np.stack([iou(ancs, gtb) for gtb in gtbs], axis=-1)\n\n  File \"<__array_function__ internals>\", line 5, in stack\n\n  File \"/user/HS127/at01053/.local/lib/python3.8/site-packages/numpy/core/shape_base.py\", line 423, in stack\n    raise ValueError('need at least one array to stack')\n\nValueError: need at least one array to stack\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_train_function_2529]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    datagen(0, 598, shuffle=True),\n",
    "    steps_per_epoch = 598,\n",
    "    epochs = 100,\n",
    "    validation_data = datagen(599, 899),\n",
    "    validation_steps = 300,\n",
    "    verbose = 1,\n",
    "    callbacks = [\n",
    "        ProgbarLogger(count_mode='steps'),\n",
    "        ModelCheckpoint('models/gtsdb_rpn-{epoch}.hdf5', verbose=1, save_best_only = True),\n",
    "        TensorBoard(log_dir='tblogs/gtsdb_rpn/', write_graph=True, write_grads=True, write_images=True),\n",
    "        EarlyStopping(patience=5, verbose=1),\n",
    "    ],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
